"""
Author: Gurunath Parasaram
Course: CS 6966 - Local Explanations for Deep Learning by Prof. Ana MarasoviÄ‡

Acknowledgements:
This script has been loosely based on the Text Classification tutorial from Huggingface
here: https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb#scrollTo=s_AY1ATSIrIq
and also other Huggingface resources.
"""

# Imports

# Standard library imports
import argparse
import jsonlines
import logging
import os
import random


# Third-party library imports
import datasets
from datasets import load_dataset, load_metric, DatasetDict
import evaluate
from IPython.display import display, HTML
import numpy as np
import pandas as pd
import torch
from torch import nn
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

parser = argparse.ArgumentParser()
parser.add_argument('--output_dir', type=str, help='Directory where model checkpoints will be saved')
args = parser.parse_args()

# Set logging config
logging.basicConfig(filename='train.log', filemode='w', level=logging.INFO, format='%(process)d-%(levelname)s-%(message)s')
logger = logging.getLogger()
 
# Setting the threshold of logger to DEBUG
logger.setLevel(logging.INFO)

#######################################
#             Helper functions        #
#######################################

# Helper functions at the start
# Helper function to display random elements
def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)

    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    # display(HTML(df.to_html()))

# Function to tokenize using HF's tokenizer
def preprocess_function(examples):
    return tokenizer(examples["text"])

# Function to evaluate accuracy
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy_metric.compute(predictions=predictions, references=labels)


#######################################
#             Main code               #
#######################################

# Get IMDB dataset from HF
logger.info(f"Loading IMDB dataset")
dataset = load_dataset("imdb")

# REFERENCE: https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090/13

# IMDB dataset just has train:test splits in 25k:25k ratio
# Create validation set from training set
# Split train test into train+validation sets in 80:20 ratio
logger.info(f"Split train test into train+validation sets in 80:20 ratio")
train_plus_validation = dataset["train"].train_test_split(test_size=0.2)
dataset = DatasetDict({
    'train': train_plus_validation['train'],
    'validation': train_plus_validation['test'],
    'test': dataset["test"],
})
# format = {'type': 'torch', 'format_kwargs' :{'dtype': torch.float}}
# dataset.set_format(**format)

dataset.set_format(type='torch', columns=['label'], format_kwargs={"dtype":torch.float64})
# Set evaluation metrics from HF evaluate to "accuracy"
accuracy_metric = evaluate.load("accuracy")



# Preprocess the dataset using HF's tokenizer using the Fast tokenizer

model_checkpoint = "microsoft/deberta-v3-base"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True) 



# Convert tokens to token_ids
logger.info(f"Tokenizing data . . . . ")
encoded_dataset = dataset.map(preprocess_function, batched=True, load_from_cache_file=False)

# Specify sequence classification model and its config
num_labels = 1

# Specify trainer + args + hyper-params
metric_name = "accuracy"
model_name = model_checkpoint.split("/")[-1]
task="imdb_classification"
batch_size = 2
validation_key = "validation"

output_dir_path = os.path.join(args.output_dir, f"{model_name}-finetuned-{task}")

model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

args = TrainingArguments(
    output_dir_path,
    evaluation_strategy = "epoch",
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=1,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name,
    push_to_hub=False,
)



class ClassificationTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss


logger.info(f"Training args:{args}")
# Piggyback on HF's Trainer code
trainer = ClassificationTrainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

logger.info(f"Starting training")
# Finally...train the model!
trainer.train()

# Evaluate the trainer model  
predictions, labels, final_metrics = trainer.predict(encoded_dataset["test"])
print(f"Final Test metrics:{final_metrics}")
outputs = []


with open(f"{output_dir_path}/test_outputs.json", "w") as op_file:
    for data_idx, data_sample in enumerate(dataset["test"]):
        outputs.append(
            "review": data_sample["text"],
            "label": labels[data_idx],
            "predicted": prediction[data_idx],
        )

with jsonlines.open(f"{output_dir_path}/outputs.jsonl"), mode='w') as writer:
    for sample in outputs:
        writer.write(sample)        

